- 对于回归问题，标签必定是连续型的，我们要让标签的数据分布更接近于正态分布，如果发现标签的峰态系数很高，并且严重偏斜，可以对标签进行对数转换。线性模型处理回归问题，目标值服从正态分布才能发挥最大的作用。



<img src="C:\Users\Liang\AppData\Roaming\Typora\typora-user-images\1571746471085.png" alt="1571746471085" style="zoom:80%;" />

<img src="C:\Users\Liang\AppData\Roaming\Typora\typora-user-images\1571746447544.png" alt="1571746447544" style="zoom:80%;" />

转换后

<img src="C:\Users\Liang\AppData\Roaming\Typora\typora-user-images\1571746513866.png" alt="1571746513866" style="zoom:80%;" />

- 对于回归问题，删除所有的离群点是不好的，因为测试集中可能有离群点，而且部分的离群点可以使得模型的健壮性更好， 相比于删除所有的离群点，我们会更在乎构建一个鲁棒性很好的模型来拟合数据。 

- 对于离群点的删除问题：找到的离群点，一定要有很好的业务解释，才能将其删掉

- 对于数据量不是特别大的数据集，可以使用可视化的方式观察离群点，如散点图，箱线图

- 特征工程的时候，训练集和测试集一块处理
- 可以使用聚合的方式进行缺失值的填充

- 对于连续型的特征，缺失值填充为0，在实际情况中可以表示，没有的意思。

-  有一些本来就应该是类别型特征，而数据集中是数值类型，所以要将其转化为字符创类型 ，或者相反

- 画出热力图之后，如果一个特征和另一个特征之间由很强的相关性，那么这个特征的一切操作可与另一个挂钩了，如一个特征的缺失值，可以用另一个特征的对应值进行填充

- 思考：为什么决策树要把熵增益越大的特征放到越前面，联想哈夫尔曼树，往往越起到主导作用的东西越作为结点