# 特征选择

## 1.0 特征选择介绍

### 1.1 特征按重要性分类

- 相关特征

  对模型有帮助，可以提升算法效果

- 无关特征

  对模型没有任何帮助

- 冗余特征

  不会给算法带来新的信息，或者**这个特征是可以由其他特征推出来的，或者是经过本特征进行运算的出来的**

### 1.2 特征选择的目的

找出对算法有益的特征，增加模型可解释性，提升模型的泛化能力，降低算法复杂度，减小训练时间



### 1.3 特征选择的原则

- 获取尽可能小的特征子集
- 不显著地降低分类精度
- 不影响分类分布

## 2.0 特征选择的方法

### 2.1 过滤法

先进行特征选择得出特征子集，在用特征子集去训练模型，==过滤法的特征选择的过程和模型无关==

**主要思想**：对每一维特征"打分"，即给每一维的特征赋予权重，这个权重代表特征的重要性，然后依据权重进行排序

**主要方法**：

- 卡方检验
- 信息增益
- 相关系数

**优点**：运算速度快

**缺点**：无法提供反馈，特征选择得好与坏的标准的评估是建立在算法获得的结果上的，这样就没办法得知算法对特征的真正需求。可能出现某个特征在原特征中显示不重要，但是该特征与其他特征结合起来变得很重要。

### 2.2 嵌入法

将特征选择嵌入到模型训练当中，可能是训练相同的模型，特征选择完成后，可以给出特征选择完成的特征和模型训练的超参数

主要思想：在模型给定的情况下学习出对模型最有用的特征

主要方法：用带有L1正则化的项完成特征选择（也可以结合L2惩罚项来优化）、随机森林平均不纯度减少法/平均精确度减少法。

优点：对特征的搜索围绕算法展开，训练次数小于封装法

### 2.3 封装法

==直接把最后要使用的分类器作为特征选择的评价函数，来对特定的分类器选择最优的特征子集==

**主要思想**：对特征子集的选择看作是一个搜索寻优的问题，对特征生成不同的组合，对这些组合进行评估，最后把这些组合放到一起进行评估

**主要方法**：递归消除特征

**优点**：对特征进行选择是围绕着算法进行的，是根据算法学习的需求进行的，可以确定最佳的特征子集。

**缺点**：速度慢

## 3.0 过滤法

### 3.1 特征选择实现方法1--方差检验

此方法一般作为在特征选择之前的一个预处理，去掉取值变化很小的特征，可以给定一个阈值，抛弃方差小于某个阈值的特征

**实现原理**

- 离散型变量：

  对于离散型变量，如果某一个值占比超过95%，就可以认为这个特征作用不大

- 连续型变量：

  需要将连续型变量离散化之后才能用

代码实现

```python
from sklearn.feature_selection import VarianceThreshold
X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
sel.fit_transform(X)
＃array([[0, 1],
       [1, 0],
       [0, 0],
       [1, 1],
       [1, 0],
       [1, 1]])
```

### 3.2 特征选择实现方法2--单变量特征选择

单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。**该方法简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）**

#### 3.2.1 Pearson 相关系数

Pearson相关系数可以衡量变量之间的线性相关性

![img](https://mmbiz.qpic.cn/mmbiz_png/jA1wO8icw0gDRSPHmzrIq5YdbIAJx04HFo9BYG4ROWtAwXrMP8ERsd3aqjRKicdvaWqZj5Pmsk859wadpFOxWwxw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- 用x_i、x_j的协方差除以x_i的标准差和x_j的标准差，**可以看成一种剔除了两个变量量纲影响、标准化后的特殊协方差。**

- **协方差是度量各个维度偏离其均值的程度**，协方差的值为正值时说明两者是正相关，否则是负相关的。

  结果的取值区间为[-1，1]，-1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关，绝对值表示相关性的强度。

- 标准差也称均方差，是方差的算术平方根，能反映一个数据集的离散程度。

**主要用于连续型特征的筛选，不适用于离散型特征的筛选**

优点：相关系数计算速度快，一般在经过数据清洗和特征提取之后就第一时间执行

缺点：使用相关系数对特征进行排序，只对线性关系敏感，如果关系是非线性的，即使两个变量具有对应关系，相关系数也会接近于0。

插话--**如何判断数据集是线性还是非线性的**

https://blog.csdn.net/weixin_42137700/article/details/86060381

https://blog.csdn.net/laoxuan2011/article/details/52786483

代码实现

```python
import numpy as np
from scipy.stats import pearsonr

np.random.seed(2019)
size=1000
x = np.random.normal(0, 1, size)
# 计算两变量间的相关系数
print("Lower noise {}".format(pearsonr(x, x + np.random.normal(0, 1, size))))
print("Higher noise {}".format(pearsonr(x, x + np.random.normal(0, 10, size))))
```

#### 3.2.2 互信息和最大信息系数

如果变量不是独立的,那么我们可以通过考察联合概率分布与边缘概率分布乘积之间的 Kullback-Leibler 散度来判断它们是否“接近”于相互独立。

##### 3.2.2.1 互信息法

**熵H(Y)与条件熵H(Y|X)**之间的差称为互信息，互信息与条件熵之间的关系：

![img](https://mmbiz.qpic.cn/mmbiz_png/jA1wO8icw0gDRSPHmzrIq5YdbIAJx04HFehdt97qE2uekgPubTrRfgKk10xGInwoIhQfRPkujqlqOCLibicJ1ibrBw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**这和ID3决策树的分支特征的选择规则是一样的**

互信息法也是评价定性自变量对定性因变量的相关性的，但是并不方便直接用于特征选择：

- 它不属于度量方式，也没有办法进行归一化，在不同的数据上的结果无法做比较。
- 只能用于离散型特征的选择，连续型特征需要先进行离散化才能用互信息进行特征选择，而互信息的结果对离散化的方式很敏感。

##### 3.2.2.2 最大信息系数方法

因为互信息法不方便直接用于特征选择，所以引入最大信息系数

最大信息系数首先是找一种最优的离散方法，把互信息的取值作为度量的方法，取值区间为[0, 1]。

代码实现

```python
x = np.random.normal(0,10,300)
z = x *x
pearsonr(x,z)
# 输出-0.1
from minepy import MINE
m = MINE()
m.compute_score(x, z)
print(m.mic())
# 输出1.0	
```

#### 3.2.3距离相关系数

距离相关系数就是为了客服Pearson相关系数的弱点而生的

原理：

![img](https://mmbiz.qpic.cn/mmbiz_png/jA1wO8icw0gDRSPHmzrIq5YdbIAJx04HF8TGcRT3PmhAcSYKeMbSPH0gvkG4jHSPiaPpMKO9iabrLyE6JknP7cG8A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**即使Pearson相关系数是0，我们也不能就以此判定两个变量是独立的，因为两个变量可能是非线性相关的。**

代码实现

```python
from scipy.spatial.distance import pdist, squareform
import numpy as np

from numbapro import jit, float32

def distcorr(X, Y):
    """ Compute the distance correlation function

    >>> a = [1,2,3,4,5]
    >>> b = np.array([1,2,9,4,4])
    >>> distcorr(a, b)
    0.762676242417
    """
    X = np.atleast_1d(X)
    Y = np.atleast_1d(Y)
    if np.prod(X.shape) == len(X):
        X = X[:, None]
    if np.prod(Y.shape) == len(Y):
        Y = Y[:, None]
    X = np.atleast_2d(X)
    Y = np.atleast_2d(Y)
    n = X.shape[0]
    if Y.shape[0] != X.shape[0]:
        raise ValueError('Number of samples must match')
    a = squareform(pdist(X))
    b = squareform(pdist(Y))
    A = a - a.mean(axis=0)[None, :] - a.mean(axis=1)[:, None] + a.mean()
    B = b - b.mean(axis=0)[None, :] - b.mean(axis=1)[:, None] + b.mean()

    dcov2_xy = (A * B).sum()/float(n * n)
    dcov2_xx = (A * A).sum()/float(n * n)
    dcov2_yy = (B * B).sum()/float(n * n)
    dcor = np.sqrt(dcov2_xy)/np.sqrt(np.sqrt(dcov2_xx) * np.sqrt(dcov2_yy))
    return dcor
```

#### 3.2.4 基于学习模型的特征排序

思路是直接使用要用的机器学习算法，对每个单独的特征和要探究的某个响应变量(要探究的变量作为因变量)之间建立预测模型，对于特征与响应变量的关系是线性的可以使用前面的方法，如果是非线性的，可以使用树模型，因为树模型可以很好的模拟非线性关系，不需要太多的调整。但是要避免的主要是过度拟合，因此树的深度应该相对较小，并且应该应用交叉验证。

#### 代码实现

```python
from sklearn.cross_validation import cross_val_score, ShuffleSplit
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor

#Load boston housing dataset as an example
boston = load_boston()
X = boston["data"]
Y = boston["target"]
names = boston["feature_names"]

rf = RandomForestRegressor(n_estimators=20, max_depth=4)
scores = []
# 使用每个特征单独训练模型，并获取每个模型的评分来作为特征选择的依据。for i in range(X.shape[1]):
     score = cross_val_score(rf, X[:, i:i+1], Y, scoring="r2",
                              cv=ShuffleSplit(len(X), 3, .3))
     scores.append((round(np.mean(score), 3), names[i]))
print(sorted(scores, reverse=True))

# 输出：[(0.636, 'LSTAT'), (0.59, 'RM'), (0.472, 'NOX'), (0.369, 'INDUS'),
(0.311, 'PTRATIO'), (0.24, 'TAX'), (0.24, 'CRIM'), (0.185, 'RAD'),
(0.16, 'ZN'), (0.087, 'B'), (0.062, 'DIS'), (0.036, 'CHAS'), (0.027, 'AGE')]
```

#### 3.2.5 卡方检验

卡方值描述的是**两个事件之间的独立性**或者描述实际观测值与期望值的偏离程度，卡方值越大，表名实际观察值与期望值偏离越大，也说明两个事件的相互独立性越弱。

原理：

![img](https://mmbiz.qpic.cn/mmbiz_png/jA1wO8icw0gDRSPHmzrIq5YdbIAJx04HFpelhJEkPCu7jfNau4RNoJmonnTwqJzx82BUWQzPObY9jxlbLwFUxmA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

CHI值(卡方值)用于衡量实际值与理论值的差异程度，除以T是为了避免不同观察值与不同期望之间产生的偏差因T的不同而差别太大，所以除以E以消除这种弊端。

- 实际值与理论值偏差的绝对大小（由于平方的存在，差异被放大）
- 差异值与理论值的相对大小

流程：

1. 计算每个特征 x~i~ 与分类变量 y之间的卡方值，进行排序
2. 设定合适的阈值，进行特征子集筛选

==只适用于 **分类问题** 中的 **离散型特征** 的筛选==

代码实现：

- sklearn.feature_selection.SelectKBest：

  返回k个最佳特征

- sklearn.feature_selection.SelectPercentile：

  返回表现最佳的前r%个特征

```python
#导入sklearn库中的SelectKBest和chi2
from sklearn.feature_selection import SelectKBest ,chi2
#选择相关性最高的前5个特征
X_chi2 = SelectKBest(chi2, k=5).fit_transform(X, y)
X_chi2.shape
输出：(27, 5)
```

### 3.3 特征选择实现方法3--线性模型与正则化

**主要思想**：当所有特征在相同尺度上时，越重要的特征应该有越高的系数，而与输出变量不想关的特征的系数应该接近于0

#### 3.3.1 L1正则化

在损失函数上加上L1正则项，由于正则项非零，迫使影响模型若的特征对应的系数变成0，因此L1正则化往往会使得很多系数为0

**Lasso能够挑出一些优质特征，同时让其他特征的系数趋于0。当如需要减少特征数的时候它很有用，但是对于数据理解来说不是很好用。**  



#### 3.3.2 L2正则化

在损失函数上加上L2正则项

- 由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。
- 对于关联特征，这意味着他们能够获得更相近的对应系数。
- Ridge将回归系数均匀的分摊到各个关联变量上。

**L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。**



原理：

多元线性回归就是要求得其中的θ，每个特征都有对应的权重系数coef，特征的权重系数的正负值代表特征与目标值是正相关还是负相关，特征的权重系数的绝对值代表重要性。



代码实现：

L1正则化

```python
#A helper method for pretty-printing linear models
def pretty_print_linear(coefs, names = None, sort = False):
    if names == None:
        names = ["X%s" % x for x in range(len(coefs))]
    lst = zip(coefs, names)
    if sort:
        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))
    return " + ".join("%s * %s" % (round(coef, 3), name)
                                   for coef, name in lst)

from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_boston

boston = load_boston()
scaler = StandardScaler()
X = scaler.fit_transform(boston["data"])
Y = boston["target"]
names = boston["feature_names"]

lasso = Lasso(alpha=.3)
lasso.fit(X, Y)

print("Lasso model: {}".format(
      pretty_print_linear(lasso.coef_, names, sort = True)))
# 输出：Lasso model: -3.707 * LSTAT + 2.992 * RM + -1.757 * PTRATIO
+ -1.081 * DIS + -0.7 * NOX + 0.631 * B + 0.54 * CHAS + -0.236 * CRIM
+ 0.081 * ZN + -0.0 * INDUS + -0.0 * AGE + 0.0 * RAD + -0.0 * TAX
```

**L1正则化回归的稳定性与非正则化线性模型类似，这意味着当数据中存在相关特征时，系数（以及特征等级）即使在小数据变化时也会发生显着变化。**



L2正则化

```python
from sklearn.linear_model import Ridge
from sklearn.metrics import r2_score
size = 100

#We run the method 10 times with different random seeds
for i in range(10):
    print("Random seed {}".format(i))
    np.random.seed(seed=i)
    X_seed = np.random.normal(0, 1, size)
    X1 = X_seed + np.random.normal(0, .1, size)
    X2 = X_seed + np.random.normal(0, .1, size)
    X3 = X_seed + np.random.normal(0, .1, size)
    Y = X1 + X2 + X3 + np.random.normal(0, 1, size)
    X = np.array([X1, X2, X3]).T


    lr = LinearRegression()
    lr.fit(X,Y)
    print("Linear model: {}".format(pretty_print_linear(lr.coef_)))

    ridge = Ridge(alpha=10)
    ridge.fit(X,Y)
    print("Ridge model: {}".format(pretty_print_linear(ridge.coef_)))

# 输出
Random seed 0
Linear model: 0.728 * X0 + 2.309 * X1 + -0.082 * X2
Ridge model: 0.938 * X0 + 1.059 * X1 + 0.877 * X2

Random seed 1
Linear model: 1.152 * X0 + 2.366 * X1 + -0.599 * X2
Ridge model: 0.984 * X0 + 1.068 * X1 + 0.759 * X2

Random seed 2
Linear model: 0.697 * X0 + 0.322 * X1 + 2.086 * X2
Ridge model: 0.972 * X0 + 0.943 * X1 + 1.085 * X2

Random seed 3
Linear model: 0.287 * X0 + 1.254 * X1 + 1.491 * X2
Ridge model: 0.919 * X0 + 1.005 * X1 + 1.033 * X2

Random seed 4
Linear model: 0.187 * X0 + 0.772 * X1 + 2.189 * X2
Ridge model: 0.964 * X0 + 0.982 * X1 + 1.098 * X2

Random seed 5
Linear model: -1.291 * X0 + 1.591 * X1 + 2.747 * X2
Ridge model: 0.758 * X0 + 1.011 * X1 + 1.139 * X2

Random seed 6
Linear model: 1.199 * X0 + -0.031 * X1 + 1.915 * X2
Ridge model: 1.016 * X0 + 0.89 * X1 + 1.091 * X2

Random seed 7
Linear model: 1.474 * X0 + 1.762 * X1 + -0.151 * X2
Ridge model: 1.018 * X0 + 1.039 * X1 + 0.901 * X2

Random seed 8
Linear model: 0.084 * X0 + 1.88 * X1 + 1.107 * X2
Ridge model: 0.907 * X0 + 1.071 * X1 + 1.008 * X2

Random seed 9
Linear model: 0.714 * X0 + 0.776 * X1 + 1.364 * X2
Ridge model: 0.896 * X0 + 0.903 * X1 + 0.98 * X2
```

对于L2正则化模型，系数非常稳定并且密切反映数据的生成方式（所有系数接近1）。

### 3.4 特征选择实现方法4--随机森林选择

随机森林准确率高，鲁棒性好，易于使用，随机森林提供了两种特征选择的方法：mean decrease impurity和mean decrease accuracy。

#### 3.4.1 平均不纯度减小 mean decrease impurity

原理：

- 当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的标准。

- 随机森林基于不纯度的排序结果非常鲜明，在得分最高的几个特征之后的特征，得分急剧的下降。

代码实现

```python
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor
import numpy as np

#Load boston housing dataset as an example
boston = load_boston()
X = boston["data"]
Y = boston["target"]
names = boston["feature_names"]
# 训练随机森林模型，并通过feature_importances_属性获取每个特征的重要性分数。rf = RandomForestRegressor()
rf.fit(X, Y)
print("Features sorted by their score:")
print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names),
             reverse=True))
```



#### 3.4.2 平均精确度减小 mean decrease accuracy

原理：

​	通过直接测量每个特征对模型精确率的影响来进行特征选择



主要思路：

打乱每个特征的顺序，测量顺序变动对模型的精确率的影响，对于不重要的特征来说，打乱顺序对模型的精确率影响不会太大，对重要的特征来说，打乱顺序会降低模型的精确率



代码实现：

```python
from sklearn.cross_validation import ShuffleSplit
from sklearn.metrics import r2_score
from collections import defaultdict
X = boston["data"]
Y = boston["target"]
rf = RandomForestRegressor()
scores = defaultdict(list)
#crossvalidate the scores on a number of different random splits of the data
for train_idx, test_idx in ShuffleSplit(len(X), 100, .3):
    X_train, X_test = X[train_idx], X[test_idx]
    Y_train, Y_test = Y[train_idx], Y[test_idx]
    # 使用修改前的原始特征训练模型，其acc作为后续混洗特征值后的对比标准。r = rf.fit(X_train, Y_train)
     acc = r2_score(Y_test, rf.predict(X_test))
     # 遍历每一列特征
    for i in range(X.shape[1]):
        X_t = X_test.copy()
        # 对这一列特征进行混洗，交互了一列特征内部的值的顺序
        np.random.shuffle(X_t[:, i])
        shuff_acc = r2_score(Y_test, rf.predict(X_t))
        # 混洗某个特征值后，计算平均精确度减少程度。scores[names[i]].append((acc-shuff_acc)/acc)
print("Features sorted by their score:")
print(sorted([(round(np.mean(score), 4), feat) for feat, score in scores.items()], reverse=True))
```

### 3.5 特征选择实现方法5--顶层特征选择

顶层特征选择发建立在基于模型的特征选择方法基础之上的，例如线性回归和SVM等，在不同的子集上建立模型，然后汇总最终确定特征得分。

#### 3.5.1 稳定性选择 Stability selection

稳定性选择常常是一种既能够有助于理解数据又能够挑出优质特征的一种选择。

原理：

- 稳定性选择是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。
- **它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果。比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。**
- 理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。



代码实现

```python
from sklearn.linear_model import RandomizedLasso
from sklearn.datasets import load_boston
boston = load_boston()
#using the Boston housing data.
#Data gets scaled automatically by sklearn's implementation
X = boston["data"]
Y = boston["target"]
names = boston["feature_names"]
rlasso = RandomizedLasso(alpha=0.025)
rlasso.fit(X, Y)
print("Features sorted by their score:")
print(sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), names),
             reverse=True))
```

#### 3.5.2 递归特征消除 Recursive feature elimination，RFE

原理：

- **递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一遍，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。**

- 这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。

- RFE的稳定性很大程度上取决于在迭代的时候底层用哪种模型。

- - 假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的。
  - 假如RFE采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。

代码实现：

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
boston = load_boston()
X = boston["data"]
Y = boston["target"]
names = boston["feature_names"]
#use linear regression as the model
lr = LinearRegression()
#rank all features, i.e continue the elimination until the last one
rfe = RFE(lr, n_features_to_select=1)
rfe.fit(X,Y)
print("Features sorted by their rank:")
print(sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names)))
结果输出
Features sorted by their rank:
[(1, 'NOX'), (2, 'RM'), (3, 'CHAS'), (4, 'PTRATIO'), (5, 'DIS'),
(6, 'LSTAT'), (7, 'RAD'), (8, 'CRIM'), (9, 'INDUS'), (10, 'ZN'),
(11, 'TAX'), (12, 'B'), (13, 'AGE')]
```

## 4.0 总结

1. 单变量特征选择可以用于理解数据、数据的结构、特点，也可以用于排除不相关特征，但是它不能发现冗余特征。

2. 正则化的线性模型可用于特征理解和特征选择。相比起L1正则化，L2正则化的表现更加稳定，L2正则化对于数据的理解来说很合适。由于响应变量和特征之间往往是非线性关系，可以采用basis  expansion的方式将特征转换到一个更加合适的空间当中，在此基础上再考虑运用简单的线性模型。

3. 随机森林是一种非常流行的特征选择方法，它易于使用。但它有两个主要问题：

4. - 重要的特征有可能得分很低（关联特征问题）
   - 这种方法对特征变量类别多的特征越有利（偏向问题）

5. 特征选择在很多机器学习和数据挖掘场景中都是非常有用的。在使用的时候要弄清楚自己的目标是什么，然后找到哪种方法适用于自己的任务。

6. - 当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。
   - 当用特征选择的方法来理解数据的时候要留心，特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。
   - 对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据。

7. 关于训练模型的特征筛选，个人建议的实施流程 :

8. 1. 数据预处理后，先排除取值变化很小的特征。如果机器资源充足，并且希望尽量保留所有信息，可以把阈值设置得比较高，或者只过滤离散型特征只有一个取值的特征。
   2. 如果数据量过大，计算资源不足（内存不足以使用所有数据进行训练、计算速度过慢），可以使用单特征选择法排除部分特征。这些被排除的特征并不一定完全被排除不再使用，在后续的特征构造时也可以作为原始特征使用。
   3. 如果此时特征量依然非常大，或者是如果特征比较稀疏时，可以使用PCA（主成分分析）和LDA（线性判别）等方法进行特征降维。
   4. 经过样本采样和特征预筛选后，训练样本可以用于训练模型。但是可能由于特征数量比较大而导致训练速度慢，或者想进一步筛选有效特征或排除无效特征（或噪音），我们可以使用正则化线性模型选择法、随机森林选择法或者顶层特征选择法进一步进行特征筛选。

**最后，特征筛选是为了理解数据或更好地训练模型，我们应该根据自己的目标来选择适合的方法。为了更好/更容易地训练模型而进行的特征筛选，如果计算资源充足，应尽量避免过度筛选特征，因为特征筛选很容易丢失有用的信息。如果只是为了减少无效特征的影响，为了避免过拟合，可以选择随机森林和XGBoost等集成模型来避免对特征过拟合。**  