### 数据采集

- 哪些数据对最后的预测结果有用
- 数据能否采集到

### 数据清洗

#### 数据清洗----思考角度

- 单维度
  - 数值是否超出了常识范围
- 统计方法
  - 箱线图--上下界
- 缺失值处理

##### 数据采样

分类问题中很多情况下正负样本不均衡，大多数模型对正负样本比较敏感（如LR）

正负样本不平衡的处理方法：

​	正样本 >> 负样本，量都很大

- 下采样

​	正样本 >> 负样本，量都不大

- 采集更多数据
- 修改损失函数

### 特征工程

#### 数值型

- 缩放处理
- 离散化
- 标准化

##### 类别型

- 独热编码
- hash与聚类处理
- 统计类别比例，转为数值型
- 分桶映射

##### 时间型

时间型既可以看做连续值，也可以看做离散值

- 连续值
  1. 持续时间
  2. 间隔时间
- 离散值
  1. 一天中哪个时间段
  2. 一周中星期几
  3. 一年中哪个星期
  4. 一年中哪个季度
  5. 工作日/周末

##### 文本型

- 词袋
- TF-IDF
- word2vec

##### 其他类型

- 简单组合特征：拼接  x1 = x2 & x3

- 模型特征组合

  ​	用树模型如GBDT产出特征组合路径

### 特征选择方法

#### filter过滤法

- 去除相关性很高的特征
- 降噪，减小噪声对模型的影响

- 按照相关程度进行排序，留下相关性高的特征
- Pearson相关系数，互信息，距离相关性
- 缺点：没有考虑到特征之间的关联作用，可能吧有用的关联特征误删

#### 包裹法 wrapper

#### 嵌入法 Embarked

### 模型的选择与调参



k-folds：固定测试集不能动，把训练集分为很多份，每次拿一份测试，其他拿去训练，如此反复

网格搜索 GridSearch：网格搜索+交叉验证 = 最佳参数，结合热力图，横轴和纵轴分别为两个参数，画出热力图

## 数据预处理



### 缺失值处理

一个**特征缺失的比率很小**，可以考虑删除对应样本或使用均值来填补

一个特征缺失有20％，并且我们知道这个特征对于模型来说是一个**很重要的因素**，就要进行填补，具体采用什么样的手段继续填补，就要理解业务，比如评分卡中对于收入这个属性，没填的可能是因为心虚，怕自己借不到钱而没填，那么这一类就可以评估为低收入人群，用四分位数进行填补；也可能是银行统计时候的失误。

随机森林填补缺失值：对于某一个特征大量缺失，其他特征却很完整的情况，非常适用。

随机森林填补缺失值代码

```python
def fill_missing_rf(X,y,to_fill):

    """
    使用随机森林填补一个特征的缺失值的函数

    参数：
    X：要填补的特征矩阵
    y：完整的，没有缺失值的标签
    to_fill：字符串，要填补的那一列的名称
    """

    #构建我们的新特征矩阵和新标签
    df = X.copy()
    fill = df.loc[:,to_fill]
    df = pd.concat([df.loc[:,df.columns != to_fill],pd.DataFrame(y)],axis=1)

    # 找出我们的训练集和测试集
    Ytrain = fill[fill.notnull()]
    Ytest = fill[fill.isnull()]
    Xtrain = df.iloc[Ytrain.index,:]
    Xtest = df.iloc[Ytest.index,:]

    #用随机森林回归来填补缺失值
    from sklearn.ensemble import RandomForestRegressor as rfr
    rfr = rfr(n_estimators=100)
    rfr = rfr.fit(Xtrain, Ytrain)
    Ypredict = rfr.predict(Xtest)

    return Ypredict


%time
X = data.iloc[:,1:]
y = data["SeriousDlqin2yrs"]#y = data.iloc[:,0]
X.shape#(149391, 10)

#=====[TIME WARNING:1 min]=====#
y_pred = fill_missing_rf(X,y,"MonthlyIncome")

#注意可以通过以下代码检验数据是否数量相同
# y_pred.shape ==  data.loc[data.loc[:,"MonthlyIncome"].isnull(),"MonthlyIncome"].shape

#确认我们的结果合理之后，我们就可以将数据覆盖了
data.loc[data.loc[:,"MonthlyIncome"].isnull(),"MonthlyIncome"] = y_pred

data.info()
```



### 异常值处理

#### 描述性统计处理异常值 pd.describe()

对于异常值，我们要把它捕捉出来，并观察它的性质，注意，我们并不是要排除掉所有异常值，相反很多时候，异常值是我们的重点研究对象，比如双十一销量超高的品牌，是我们观察的重点。

找出对于业务来说不符合常理的数值，比如收入为负的。

日常处理异常值，我们使用**箱线图**或者**3σ法则**来找到异常值，在特征有限的情况下，可以使用描述性统计pd.describe()来观察，如果有几百个特征又无法成功降维或特征选择不管用，那还是用**3σ法则**比较好。



### 数值转换

不一定要同一量纲 和 标准化数据分布，根据业务需求，

### 样本不平衡问题

**逻辑回归**中使用最多的是用采样方法来平衡样本。

```python
#探索标签的分布
X = data.iloc[:,1:]
y = data.iloc[:,0]
 
y.value_counts()#查看每一类别值得数据量，查看样本是否均衡
 
n_sample = X.shape[0]
 
n_1_sample = y.value_counts()[1]
n_0_sample = y.value_counts()[0]
 
print('样本个数：{}; 1占{:.2%}; 0占{:.2%}'.format(n_sample,n_1_sample/n_sample,n_0_sample/n_sample))
#样本个数：149165; 1占6.62%; 0占93.38%


#如果报错，就在prompt安装：pip install imblearn
import imblearn
#imblearn是专门用来处理不平衡数据集的库，在处理样本不均衡问题中性能高过sklearn很多
#imblearn里面也是一个个的类，也需要进行实例化，fit拟合，和sklearn用法相似
 
from imblearn.over_sampling import SMOTE
 
sm = SMOTE(random_state=42) #实例化
X,y = sm.fit_sample(X,y)
 
n_sample_ = X.shape[0]#278584

pd.Series(y).value_counts()
 
n_1_sample = pd.Series(y).value_counts()[1]
n_0_sample = pd.Series(y).value_counts()[0]
 
print('样本个数：{}; 1占{:.2%}; 0占{:.2%}'.format(n_sample_,n_1_sample/n_sample_,n_0_sample/n_sample_))
#样本个数：278584; 1占50.00%; 0占50.00%
```

### 分箱

分箱的本质，其实就是离散化连续变量，，其实本质比较类似于聚类，分箱要注意两个问题

- 分多少箱合适？

  最开始我们并不知道，但是既然是将连续型变量离散化，想也知道箱子个数必然不能太多，最好控制在十个以下。而用来制作评分卡，最好能在4~5个为最佳。我们知道，离散化连续变量必然伴随着信息的损失，并且箱子越少，信息损失越大。

- 如何评估分箱的效果？

  可以使用卡方检验来对比两个箱子之间的相似性，如果两个箱子之间卡方检验的P值很大，则说明他们非常相似，那我们就可以将这两个箱子合并为一个箱子。

  对于效果函数 func，根据不同的业务场景有不同的定义。 func 代表的意义是我们特征上的信息量以及这个特征对模型的贡献。

  基于这样的思想，我们总结出我们对一个特征进行分箱的步骤：

  1）我们首先把连续型变量分成一组数量较多的分类型变量，比如，将几万个样本分成100组，或50组

  2）确保每一组中都要包含两种类别的样本，否则IV值会无法计算

  3）我们对相邻的组进行卡方检验，卡方检验的P值很大的组进行合并，直到数据中的组数小于设定的N箱为止

  4）我们让一个特征分别分成[2,3,4.....20]箱，观察每个分箱个数下的IV值如何变化，找出最适合的分箱个数

  5）分箱完毕后，我们计算每个箱的WOE值，，观察分箱效果这些步骤都完成后，我们可以对各个特征都进行分箱，然后观察每个特征的IV值，以此来挑选特征。

  [具体参考课件](file:///C:/Users/Liang/Desktop/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E8%8F%9C%E8%8F%9C%E7%9A%84sklearn%E8%AF%BE%E5%A0%82(1-12%E5%85%A8%E8%AF%BE)/05%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E8%AF%84%E5%88%86%E5%8D%A1/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%20full%20version.pdf)

