![img](https://mmbiz.qpic.cn/mmbiz_jpg/jA1wO8icw0gCbrqv6F5kIvYemBkiah8QX8ZDeuicyIWZqMfIaxcUzVTzDbicSny9stE4Wp1ExRfAOhpWO49SlfJtfg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### 数值型特征无量纲化

**数据无量纲化的原因：**

- 某些算法要求样本具有零均值和单位方差

- 消除不同的特征之间不同量级的影响

  - 归一化可能会提升精度；

    数据的量级差异会使得数值较大的特征占有主导的地位（归一化让拟合出来的超平面更加的平滑，不会过于直，也不会过于弯曲）

  - 数量级的差异会使得迭代收敛速度减慢

    在使用梯度下降法求最优值时，容易走“之字型”路线，要迭代很多次才能到达最优值

  - 依赖于样本距离的算法对于数据的数量级非常敏感



#### 数据标准化

标准化的前提是特征值服从正态分布，标准化后转换为标准正态分布

z-score标准化对有离群数据的数据集有较好的效果

**优点：**

z-score可以应用于数值型的数据，不受数据量级的影响

**缺点：**

- z-score的需要总体的平均值和方差，大多数情况下这个真实值是很难得到的，只能用样本集的均值和方差代替
- z-score对数据的分布有一定的要求
- 消除了数据具有的实际意义
- 在存在异常值时无法保证平衡的特征尺度。

#### 归一化

##### 1）MinMax归一化

区间缩放到0和1之间

**缺点：**

- 对异常值十分敏感

##### 2）MaxAbs归一化

单独地缩放和转换每个特征，使得训练集中的每个特征的最大绝对值将为1.0，将属性缩放到[-1,1]。它不会移动/居中数据，因此不会破坏任何稀疏性。

<img src="https://mmbiz.qpic.cn/mmbiz_jpg/jA1wO8icw0gCbrqv6F5kIvYemBkiah8QX8mLSOaicbWXU7RXz1I1BTLGvLiazyn0ltwEdSsQXEJS6gU7KpaiaiaNuiabg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img" style="zoom:25%;" />

当特征值全是正数据的时候，缩放到0-1之间

**缺点：**

易受异常值影响



#### 正态分布化

没懂。。。。先放着



### 标准化与归一化对比

#### 1）标准化与归一化的异同

**相同点：**
它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。

**不同点：**

- 目的不同，归一化是为了消除纲量压缩到[0,1]区间；

  标准化只是调整特征整体的分布；

- 归一化与最大，最小值有关；

  标准化与均值，标准差有关；

- 归一化输出在[0,1]之间；

  标准化无限制。

#### 2）什么时候用归一化？什么时候用标准化？

- 如果对输出结果范围有要求，用归一化；
- 如果数据较为稳定，不存在极端的最大最小值，用归一化；
- 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。

#### 3）归一化与标准化的应用场景

- 在分类、聚类算法中，需要使用距离来度量相似性的时候（如SVM、KNN）、或者使用PCA技术进行降维的时候，标准化(Z-score standardization)表现更好；

- 在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法。

  比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围；

- 基于树的方法不需要进行特征的归一化。

  例如随机森林，bagging与boosting等方法。

  如果是基于参数的模型或者基于距离的模型，因为需要对参数或者距离进行计算，都需要进行归一化。

### 数值型特征分箱

**分箱的重要性及其优势：**

- 离散特征的增加和减少都很容易，易于模型的快速迭代；

- 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；

- 离散化后的特征对异常数据有很强的鲁棒性；

  比如一个特征是年龄>30是1，否则0。

  如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；

- 对于线性模型，表达能力受限；

  单变量离散化为N个后，每个变量有单独的权重，相当于模型引入了非线性，能够提升模型表达能力，加大拟合；

- 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；

- 特征离散化后，模型会更稳定；

  比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。

  当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；

- 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险；

- 可以将缺失作为独立的一类带入模型；

- 将所有变量变换到相似的尺度上。

#### 无督促分箱法

##### 1）自定义分享发

 根据业务、常识划分区间

##### 2）等距分箱

按照相同宽度进行分箱

缺点：受异常值的影响大

##### 3）等频分箱

##### 4）聚类分箱

##### 定义

基于k均值聚类的分箱：k均值聚类法将观测值聚为k类，但在聚类过程中需要保证分箱的有序性：第一个分箱中所有观测值都要小于第二个分箱中的观测值，第二个分箱中所有观测值都要小于第三个分箱中的观测值，等等。

##### 实现步骤

- Step 0：

  对预处理后的数据进行归一化处理；

- Step 1：

  将归一化处理过的数据，应用k-means聚类算法，划分为多个区间：

  采用等距法设定k-means聚类算法的初始中心，得到聚类中心；

- Step 2：

  在得到聚类中心后将相邻的聚类中心的中点作为分类的划分点，将各个对象加入到距离最近的类中，从而将数据划分为多个区间；

- Step 3：

  重新计算每个聚类中心，然后重新划分数据，直到每个聚类中心不再变化，得到最终的聚类结果。

```python
from sklearn.cluster import KMeans
kmodel=KMeans(n_clusters=k)  #k为聚成几类
kmodel.fit(data.reshape(len(data),1))) #训练模型
c=pd.DataFrame(kmodel.cluster_centers_) #求聚类中心
c=c.sort_values(by=’列索引') #排序　　
w=pd.rolling_mean(c,2).iloc[1:] #用滑动窗口求均值的方法求相邻两项求中点，作为边界点
w=[0] +list(w[0] + [ data.max() ]  #把首末边界点加上
d3= pd.cut(data,w,labels=range(k)) #cut函数
```

##### 5）二值化

将连续型数据二值化，这对于数据服从伯努利分布很有用



#### 有督促分箱法

##### 1）卡方分箱法

##### 定义

自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。

##### 基本思想

对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。

##### 实现步骤

- Step 0：

  预先定义一个卡方的阈值；

- Step 1：

  初始化；

  
  根据要离散的属性对实例进行排序，每个实例属于一个区间；

- Step 2：

  合并区间；

- - 计算每一对相邻区间的卡方值；
  - 将卡方值最小的一对区间合并；

<img src="https://mmbiz.qpic.cn/mmbiz_jpg/jA1wO8icw0gCbrqv6F5kIvYemBkiah8QX8S68pOnMOMABCiavy5ibzocITmT1KWLFHGtA1S9eeXof2Dtv3Lt06158Q/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img" style="zoom: 25%;" />

```
Aij：第i区间第j类的实例的数量；Eij：Aij的期望频率（=(Ni*Cj)/N），N是总样本数，Ni是第i组的样本数，Cj是第j类样本在全体中的比例；
```

##### 阈值的意义

类别和属性独立时,有90%的可能性,计算得到的卡方值会小于4.6。大于阈值4.6的卡方值就说明属性和类不是相互独立的，不能合并。如果阈值选的大,区间合并就会进行很多次,离散后的区间数量少、区间大。

##### 注意

- ChiMerge算法推荐使用0.90、0.95、0.99置信度，最大区间数取10到15之间；

- 也可以不考虑卡方阈值，此时可以考虑最小区间数或者最大区间数。

  指定区间数量的上限和下限，最多几个区间,最少几个区间；

- 对于类别型变量，需要分箱时需要按照某种方式进行排序。

##### 实现代码

https://github.com/tatsumiw/ChiMerge/blob/master/ChiMerge.py

##### 2）最小熵法分箱

需要使总熵值达到最小，也就是使分箱能够最大限度地区分因变量的各类别。

熵是信息论中数据无序程度的度量标准，提出信息熵的基本目的是找出某种符号系统的信息量和冗余度之间的关系，以便能用最小的成本和消耗来实现最高效率的数据存储、管理和传递。
数据集的熵越低，说明数据之间的差异越小，最小熵划分就是为了使每箱中的数据具有最好的相似性。给定箱的个数，如果考虑所有可能的分箱情况，最小熵方法得到的箱应该是具有最小熵的分箱。