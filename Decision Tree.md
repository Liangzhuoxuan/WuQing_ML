Decision Tree：

1. 树结构介绍
2. 决策树基本流程
   1. 学习过程
   2. 决策过程
   3. 总体流程
   4. 停止划分的条件
   5. 流程的伪代码
3. 最佳属性的选择方法
   1. 信息增益
   2. 信息增益率
   3. Gini系数
   4. 熵、Gini指数、误差率之间的关系
4. 剪枝与控制过拟合
   1. 预剪枝
   2. 后剪枝
   3. 预剪枝和后剪枝的对比
5. 连续值和缺失值的处理
   1. 连续值
   2. 缺失值
6. 回归树
   1. 回归树模型介绍
   2. 回归树构建方法
   3. 回归树剪枝
7. 随机森林
   1. 装袋法bagging
   2. 随机森林

# Decision Tree

## 1 树结构介绍

决策树基于“树”的结构进行决策

- 每个内部节点对应于某个属性上的测试
- 每个分支对应于测试上的一种结果
- 每个叶子节点对应于一个预测结果

## 2 决策树基本流程

### 2.1 学习过程

### 2.2 决策过程

### 2.3 总体流程

总体流程：采用分而治之的方式

- 由根至叶递归
- 每个中间节点寻找划分属性

### 2.4 停止划分的条件

三种停止划分的条件：

- 当前**节点**包含的全部样本属于同一类别（标签相同），无需划分
- 当前节点已经没有样本了
- 已经没有特征可以选择了，或是所有样本在所有特征上取值（标签）相同

### 2.5 流程的伪代码

![1566264224990](C:\Users\Liang\AppData\Roaming\Typora\typora-user-images\1566264224990.png)

## 3 最佳属性的选择方法

### 3.1 信息增益



### 3.2 信息增益率

### 3.3 Gini系数

### 3.4熵、Gini指数、误差率之间的关系

## 4 剪枝与控制过拟合

### 4.1 预剪枝

### 4.2 后剪枝

### 4.3 预剪枝和后剪枝的对比

## 5 连续值和缺失值的处理

### 5.1连续值

 基本思想：将连续属性离散化

常用做法：二分法

- n个属性值可以形成 n-1 个候选划分
- 把候选划分值当做离散值属性处理，寻找最佳划分

把连续型的值从小到大进行排序， 假设有n个取值，找到n-1个划分点，如 x~1~ 和 x~2~ 的中间值就是其中一个划分点，以此类推，每次选定一个划分点，小于这个划分点的数值就为0，大于这个划分点的数值就为1，在这n-1个划分点当中寻找最优的划分点（信息增益最大）

### 5.2 缺失值

如果使用带缺失值的样例，需要解决几个问题：

- 如何进行划分属性选择？
- 给定划分属性，若样本在该属性上的值缺失，如何进行划分？

基本思路：样本赋权，权重划分



<img src="C:\Users\Liang\AppData\Roaming\Typora\typora-user-images\1574923155275.png" alt="1574923155275" style="zoom:80%;" />





在划分的的时候，**仅通过无缺失值的样本来计算该样本集的信息熵，仅通过无缺失值的样本来计算特征的信息增益**

比如对于某个特征，总数时17个，缺失值有3个，无缺失的样本有14个，那么在计算这个特征和根节点的信息增益时，就要给这个信息增益乘上一个权重（置信度），这个权重的值是 $\frac{14}{17}$

对于含缺失值的样本集D，和样本集中不含缺失的样本$D'$，计算信息增益
$$
Gain(D,x_i) = ρ * Gain(D',x_i)\\
ρ = \frac{无缺失的样本数}{样本总数}
$$




## 6 回归树

### 6.1回归树模型介绍

对于一个数据集而言，有 n 个特征，每个特征代表一个维度，回归树就是在每个维度上“砍一刀”，在每个维度上进行一次二分，把这个n维空间分成不同的区域，每个区域当中选择一个值最为预估的结果。

### 6.2 回归树构建方法

1. 把整个特征空间切分成 J 个没有重叠的区域
2. 计算每个区域中的所有样本的数值的均值

**递归二分**

- 自顶向下的贪婪式递归方案
  - 自顶向下：不说了
  - 贪婪：每一次划分只考虑当前最优，不回头考虑之前的划分



在当前这一层的时候，遍历所有的特征和所有的切分点，每次遍历只去“砍一刀”，我选取的是，在我当前这个特征下，我砍下这一刀后，RSS最小，此时RSS= 第1个区域每个值与该区域均值之差的平方和 + 第2个区域每个值与该区域均值之差的平方和

### 6.3 回归树剪枝

如果让回归树充分生长，同样会有过拟合的风险

主要思想：控制叶子节点的个数

剪枝方法：给RSS添加正则化项，给正则项添加一个系数$\alpha$，

添加正则化项

![1567146698416](C:\Users\Liang\AppData\Roaming\Typora\typora-user-images\1567146698416.png)



## 7 随机森林

### 7.1 装袋法bagging

使用bagging降低过拟合风险，提高模型的泛化能力

![1566358647419](C:\Users\Liang\AppData\Roaming\Typora\typora-user-images\1566358647419.png)

### 7.2 随机森林

随机森林随机在哪里（双重随机性）？

- 做了随机的采样，在数据集构建的时候就用了又放回的采样，构建出采样集。
- 在进行训练节点（节点划分）时，从所有特征中随机选出一些特征来计算最优进行划分。

随机森林分类，最后的分类结果少数服从多数，随机森林回归，最后的回归结果取均值

为什么随机森林抗噪效果好，因为我每次是随机又放回的从数据集里面进行样本的采样，在所有树中有噪声的树就很少，而最后又是少数服从多数，所以抗噪能力很好







学习过程：划分属性

决策过程：由上到下



==**拓展**==：

纯度：里面类别一样的概率有多高 



> 为什么cart决策树就是一个二分类的决策树，ID3和C4.5就是可以多分类的呢 ？因为gini指数的定义就是，从某一个特征的数据中抽取
>
> gini只是信息度量的一种方式，效果和信息熵一样，是信息熵做了一阶泰勒展开的近似，但是gini的计算复杂度更低(没有对数运算)，CART使用二分类一定程度上也减轻了gini指数的计算，并且二分类也不容易发生类似使用信息增益或者信息增益率产生了偏向于选择取值类别更多或者更少的特征的情况。
>
> 不是说多分类就不能使用gini系数作为指标，只是使用gini系数+二分叉的方式是一种很不错的方式而已（计算快+不容易发生偏向于选择类别太多的特征的情况）

> - 一颗决策树对应于一个“规则集”
> - 每个从根节点到叶节点的分支路径对应于一条规则
>
> 所以其实每个分支都是一条判断语句
>
> ​	if a==?  and b < ?  and c > ?
>
> ​	then 0 or 1